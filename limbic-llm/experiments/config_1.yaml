

experiment_name: "limbic_base_finetune"
model:
  name: "gpt2"
  pretrained: true
  adapter: "none"

data:
  train_path: "data/processed/train.json"
  val_path: "data/processed/val.json"
  tokenizer: "gpt2"

training:
  batch_size: 8
  epochs: 3
  learning_rate: 5e-5
  max_seq_length: 512
  gradient_accumulation_steps: 2
  save_every: 1
  log_every: 50

output:
  model_dir: "models/checkpoints"
  log_dir: "experiments/results/logs"